# Reddit AI Trend Report - 2025-12-02

## Today's Trending Posts

| Title | Community | Score | Comments | Category | Posted |
|-------|-----------|-------|----------|----------|--------|
| [That is actually cheap damn](https://www.reddit.com/comments/1pbd84l) | [r/singularity](https://www.reddit.com/r/singularity) | 1351 | 276 | AI | 2025-12-01 13:48 UTC |
| [deepseek-ai/DeepSeek-V3.2 · Hugging Face](https://www.reddit.com/comments/1pb9xm3) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 934 | 183 | New Model | 2025-12-01 11:01 UTC |
| [Deepseek New Model gets Gold in IMO](https://www.reddit.com/comments/1pbaqkk) | [r/singularity](https://www.reddit.com/r/singularity) | 900 | 232 | AI | 2025-12-01 11:48 UTC |
| [Work culture at Deepseek](https://www.reddit.com/comments/1pbek1o) | [r/singularity](https://www.reddit.com/r/singularity) | 800 | 83 | AI | 2025-12-01 14:44 UTC |
| [transformers v5 is out!](https://www.reddit.com/comments/1pbl22j) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 664 | 40 | News | 2025-12-01 18:45 UTC |
| [Deepseek releases cheap general imo level model before op...](https://www.reddit.com/comments/1pbazan) | [r/singularity](https://www.reddit.com/r/singularity) | 581 | 42 | AI | 2025-12-01 12:01 UTC |
| [You can now do 500K context length fine-tuning - 6.4x longer](https://www.reddit.com/comments/1pbh87f) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 356 | 45 | Resources | 2025-12-01 16:26 UTC |
| [WebGPU Finally, it is compatible with all major browsers](https://www.reddit.com/comments/1pbs9u9) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 305 | 41 | News | 2025-12-01 23:21 UTC |
| [Kling O1 a new model that can edit videos and more](https://www.reddit.com/comments/1pbgg4f) | [r/singularity](https://www.reddit.com/r/singularity) | 298 | 44 | AI | 2025-12-01 15:57 UTC |
| [A history professor says AI didn\'t break college — it ex...](https://www.reddit.com/comments/1pby1g5) | [r/singularity](https://www.reddit.com/r/singularity) | 293 | 40 | AI | 2025-12-02 03:36 UTC |


## Weekly Popular Posts

| # | Title | Community | Score | Comments | Category | Posted |
|---|-------|-----------|-------|----------|----------|--------|
| 1 | [Throwback to Yann LeCun’s 1989 convolutional neural netwo...](https://www.reddit.com/comments/1p88l9k) | [r/singularity](https://www.reddit.com/r/singularity) | 2295 | 132 | AI | 2025-11-27 17:54 UTC |
| 2 | [Elon Musk predicted that AGI would arrive in 2025.&nbsp;N...](https://www.reddit.com/comments/1p81boq) | [r/singularity](https://www.reddit.com/r/singularity) | 1734 | 567 | AI | 2025-11-27 12:44 UTC |
| 3 | [Not sure what they are training it for.](https://www.reddit.com/comments/1p7hjv7) | [r/singularity](https://www.reddit.com/r/singularity) | 1496 | 326 | Shitposting | 2025-11-26 19:49 UTC |
| 4 | [\[D\] Got burned by an Apple ICLR paper — it was withdraw...](https://www.reddit.com/comments/1p82cto) | [r/MachineLearning](https://www.reddit.com/r/MachineLearning) | 1467 | 92 | Discussion | 2025-11-27 13:35 UTC |
| 5 | [The prompt being used to generate influencers (NanoBanana...](https://www.reddit.com/comments/1p8ppdy) | [r/singularity](https://www.reddit.com/r/singularity) | 1415 | 292 | AI | 2025-11-28 08:24 UTC |
| 6 | [That is actually cheap damn](https://www.reddit.com/comments/1pbd84l) | [r/singularity](https://www.reddit.com/r/singularity) | 1349 | 276 | AI | 2025-12-01 13:48 UTC |
| 7 | [Ilya has spoken](https://www.reddit.com/comments/1p6wdyn) | [r/singularity](https://www.reddit.com/r/singularity) | 1102 | 251 | Meme | 2025-11-26 02:47 UTC |
| 8 | [Leak: OpenAI is building an Ad Network inside ChatGPT.&nb...](https://www.reddit.com/comments/1p9nxjg) | [r/singularity](https://www.reddit.com/r/singularity) | 1067 | 268 | AI | 2025-11-29 12:36 UTC |
| 9 | [Google is finally working about fix the Gemini’s buggy UI.](https://www.reddit.com/comments/1pa59t6) | [r/singularity](https://www.reddit.com/r/singularity) | 1060 | 102 | AI | 2025-11-30 01:12 UTC |
| 10 | [\"OpenAI had a 2-year lead in the AI race to work \'uncon...](https://www.reddit.com/comments/1p6j55u) | [r/singularity](https://www.reddit.com/r/singularity) | 1031 | 222 | AI | 2025-11-25 17:48 UTC |
| 11 | [Will Smith eating spaghetti &#124; Nano Banana Pro + Grok AI](https://www.reddit.com/comments/1p8ot5q) | [r/singularity](https://www.reddit.com/r/singularity) | 980 | 186 | AI | 2025-11-28 07:26 UTC |
| 12 | [Nvidia feels threatened after Google TPU deal with Meta.](https://www.reddit.com/comments/1p6l3jx) | [r/singularity](https://www.reddit.com/r/singularity) | 947 | 136 | AI | 2025-11-25 18:59 UTC |
| 13 | [Perplexity permabanned me in their official sub for citin...](https://www.reddit.com/comments/1pavwiq) | [r/singularity](https://www.reddit.com/r/singularity) | 918 | 115 | AI | 2025-11-30 22:46 UTC |
| 14 | [Deepseek New Model gets Gold in IMO](https://www.reddit.com/comments/1pbaqkk) | [r/singularity](https://www.reddit.com/r/singularity) | 902 | 232 | AI | 2025-12-01 11:48 UTC |
| 15 | [Ilya on his interview](https://www.reddit.com/comments/1p8xbyq) | [r/singularity](https://www.reddit.com/r/singularity) | 800 | 232 | AI | 2025-11-28 15:16 UTC |
| 16 | [Work culture at Deepseek](https://www.reddit.com/comments/1pbek1o) | [r/singularity](https://www.reddit.com/r/singularity) | 792 | 83 | AI | 2025-12-01 14:44 UTC |
| 17 | [You can now do FP8 reinforcement learning locally! (<5GB ...](https://www.reddit.com/comments/1p6k0h2) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 742 | 83 | Resources | 2025-11-25 18:19 UTC |
| 18 | [Any idea when RAM prices will be “normal”again?](https://www.reddit.com/comments/1pa85la) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 733 | 280 | Question | Help | 2025-11-30 03:36 UTC |
| 19 | [ChatGPT is now 3 years old!](https://www.reddit.com/comments/1paia0n) | [r/singularity](https://www.reddit.com/r/singularity) | 714 | 84 | AI | 2025-11-30 13:27 UTC |
| 20 | [New benchmark: Claude Opus 4.5 broke the efficiency wall....](https://www.reddit.com/comments/1p9xfa5) | [r/singularity](https://www.reddit.com/r/singularity) | 692 | 101 | AI | 2025-11-29 19:25 UTC |


## Monthly Popular Posts

| # | Title | Community | Score | Comments | Category | Posted |
|---|-------|-----------|-------|----------|----------|--------|
| 1 | [People on X are noticing something interesting about Grok..](https://www.reddit.com/comments/1p22c89) | [r/singularity](https://www.reddit.com/r/singularity) | 5948 | 779 | Discussion | 2025-11-20 12:50 UTC |
| 2 | [Grok made to glaze Elon Musk](https://www.reddit.com/comments/1p22hml) | [r/singularity](https://www.reddit.com/r/singularity) | 4769 | 497 | Discussion | 2025-11-20 12:58 UTC |
| 3 | [Dental revolution](https://www.reddit.com/comments/1p457q1) | [r/singularity](https://www.reddit.com/r/singularity) | 4744 | 183 | Biotech/Longevity | 2025-11-22 21:49 UTC |
| 4 | [AI detector](https://www.reddit.com/comments/1p5nbua) | [r/singularity](https://www.reddit.com/r/singularity) | 3725 | 185 | Discussion | 2025-11-24 17:30 UTC |
| 5 | [Any day now](https://www.reddit.com/comments/1ox8job) | [r/singularity](https://www.reddit.com/r/singularity) | 3470 | 208 | Meme | 2025-11-14 21:05 UTC |
| 6 | [Grok lobotomised succesfully](https://www.reddit.com/comments/1p2v13q) | [r/singularity](https://www.reddit.com/r/singularity) | 3193 | 190 | AI | 2025-11-21 10:17 UTC |
| 7 | [Heretic: Fully automatic censorship removal for language ...](https://www.reddit.com/comments/1oymku1) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 2855 | 295 | Resources | 2025-11-16 14:05 UTC |
| 8 | [Xpeng\'s new humanoid/gynoid looks closer to the human form.](https://www.reddit.com/comments/1op0qwd) | [r/singularity](https://www.reddit.com/r/singularity) | 2769 | 847 | Robotics | 2025-11-05 11:50 UTC |
| 9 | [Nano Banana 2 CRAZY image outputs](https://www.reddit.com/comments/1otuefg) | [r/singularity](https://www.reddit.com/r/singularity) | 2606 | 273 | AI | 2025-11-11 00:00 UTC |
| 10 | [Gemini 3.0 Pro benchmark results](https://www.reddit.com/comments/1p095c9) | [r/singularity](https://www.reddit.com/r/singularity) | 2459 | 602 | AI | 2025-11-18 11:08 UTC |
| 11 | [Throwback to Yann LeCun’s 1989 convolutional neural netwo...](https://www.reddit.com/comments/1p88l9k) | [r/singularity](https://www.reddit.com/r/singularity) | 2290 | 132 | AI | 2025-11-27 17:54 UTC |
| 12 | [Don\'t be those guys !](https://www.reddit.com/comments/1p60se4) | [r/singularity](https://www.reddit.com/r/singularity) | 2270 | 226 | Meme | 2025-11-25 02:30 UTC |
| 13 | [Jeff Bezos\'s Blue Origin launches New Glenn rocket with ...](https://www.reddit.com/comments/1owdwj4) | [r/singularity](https://www.reddit.com/r/singularity) | 2232 | 232 | Space & Astroengineering | 2025-11-13 21:41 UTC |
| 14 | [Google is likely to win the AI race](https://www.reddit.com/comments/1p0qgg1) | [r/singularity](https://www.reddit.com/r/singularity) | 2190 | 362 | AI | 2025-11-18 22:43 UTC |
| 15 | [20,000 Epstein Files in a single text file available to d...](https://www.reddit.com/comments/1ozu5v4) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 2146 | 249 | Resources | 2025-11-17 22:14 UTC |
| 16 | [Anthropic pushing again for regulation of open source mod...](https://www.reddit.com/comments/1oximzj) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 2096 | 255 | Discussion | 2025-11-15 04:40 UTC |
| 17 | [MindOn trained a Unitree G1 to open curtains, plant care,...](https://www.reddit.com/comments/1owwfp9) | [r/singularity](https://www.reddit.com/r/singularity) | 2093 | 428 | Robotics | 2025-11-14 13:26 UTC |
| 18 | [So \"we hit a wall people\" ....&nbsp;isn\'t looking good](https://www.reddit.com/comments/1p0j7us) | [r/singularity](https://www.reddit.com/r/singularity) | 1922 | 446 | AI | 2025-11-18 18:09 UTC |
| 19 | [Peak AI](https://www.reddit.com/comments/1otfhbn) | [r/singularity](https://www.reddit.com/r/singularity) | 1882 | 240 | AI | 2025-11-10 14:39 UTC |
| 20 | [XPENG IRON - some thought she was one of us.&nbsp;So they...](https://www.reddit.com/comments/1oq6ejd) | [r/singularity](https://www.reddit.com/r/singularity) | 1755 | 329 | Robotics | 2025-11-06 18:14 UTC |


## Top Posts by Community (Past Week)

### r/AI_Agents

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [We cut agent token usage and speed by ~82% with one dumb ...](https://www.reddit.com/comments/1pbfjru) | 192 | 38 | Tutorial | 2025-12-01 15:23 UTC |
| [Why Build a Giant Model When You Can Orchestrate Experts?](https://www.reddit.com/comments/1pbcz1o) | 23 | 13 | Discussion | 2025-12-01 13:37 UTC |
| [AI for startups shouldn’t replace people.&nbsp;It should ...](https://www.reddit.com/comments/1pbj20k) | 12 | 12 | Discussion | 2025-12-01 17:33 UTC |


### r/LangChain

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [Anyone tried building a personality-based AI companion wi...](https://www.reddit.com/comments/1pbi93e) | 1 | 11 | Discussion | 2025-12-01 17:03 UTC |


### r/LocalLLaMA

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [deepseek-ai/DeepSeek-V3.2 · Hugging Face](https://www.reddit.com/comments/1pb9xm3) | 934 | 183 | New Model | 2025-12-01 11:01 UTC |
| [transformers v5 is out!](https://www.reddit.com/comments/1pbl22j) | 664 | 40 | News | 2025-12-01 18:45 UTC |
| [You can now do 500K context length fine-tuning - 6.4x longer](https://www.reddit.com/comments/1pbh87f) | 356 | 45 | Resources | 2025-12-01 16:26 UTC |


### r/MachineLearning

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [\[R\] : Is it acceptable to contact the editor after reje...](https://www.reddit.com/comments/1pbcpog) | 40 | 19 | Research | 2025-12-01 13:25 UTC |


### r/datascience

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [What worked for you for job search?](https://www.reddit.com/comments/1pbjrp6) | 23 | 23 | Discussion | 2025-12-01 17:59 UTC |
| [Model learning selection bias instead of true relationship](https://www.reddit.com/comments/1pbpnmz) | 20 | 18 | ML | 2025-12-01 21:37 UTC |
| [What do you guys think about AI\'s effect on Jobs?](https://www.reddit.com/comments/1pbi7a6) | 0 | 37 | Discussion | 2025-12-01 17:02 UTC |


### r/singularity

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [That is actually cheap damn](https://www.reddit.com/comments/1pbd84l) | 1351 | 276 | AI | 2025-12-01 13:48 UTC |
| [Deepseek New Model gets Gold in IMO](https://www.reddit.com/comments/1pbaqkk) | 900 | 232 | AI | 2025-12-01 11:48 UTC |
| [Work culture at Deepseek](https://www.reddit.com/comments/1pbek1o) | 800 | 83 | AI | 2025-12-01 14:44 UTC |




## Trend Analysis

### **1. Today's Highlights**

#### **New Model Releases and Performance Breakthroughs**
- **[DeepSeek-V3.2-Speciale Dominates Competitions](https://www.reddit.com/comments/1pbaqkk)**  
  - DeepSeek's V3.2-Speciale model achieved gold medals in IMO, CMO, ICPC World Finals, and IOI 2025, outperforming competitors like Gemini 3.0 Pro. The model excels in complex reasoning tasks but uses more tokens, making it API-only for now.  
  - *Why it matters:* This demonstrates DeepSeek's growing prowess in AI reasoning, rivaling top models like Gemini while maintaining affordability. Community reactions highlight its potential to democratize high-performance AI.  
  - Post link: [Deepseek New Model gets Gold in IMO](https://www.reddit.com/comments/1pbaqkk) (Score: 900, Comments: 232)

- **[DeepSeek-V3.2 Pricing Revolution](https://www.reddit.com/comments/1pbd84l)**  
  - DeepSeek released V3.2 with significantly reduced pricing—30x cheaper than Gemini 3.0 Pro. Cache hit inputs now cost $0.028 (down from $0.07), and outputs are $0.42 (down from $1.68).  
  - *Why it matters:* This pricing strategy disrupts the AI market, making high-performance models more accessible and challenging competitors like Google and OpenAI.  
  - Post link: [That is actually cheap damn](https://www.reddit.com/comments/1pbd84l) (Score: 1351, Comments: 276)

- **[500K Context Length Fine-Tuning Breakthrough](https://www.reddit.com/comments/1pbh87f)**  
  - A new method enables 500K context length fine-tuning, 6.4x longer than previous limits, using 72% less VRAM. This advancement allows local training of models like GPT-OSS-20B on a single GPU.  
  - *Why it matters:* This breakthrough democratizes long-context training, enabling smaller organizations and individuals to achieve state-of-the-art results without massive computational resources.  
  - Post link: [You can now do 500K context length fine-tuning - 6.4x longer](https://www.reddit.com/comments/1pbh87f) (Score: 356, Comments: 45)

- **[WebGPU Compatibility Across Browsers](https://www.reddit.com/comments/1pbs9u9)**  
  - WebGPU is now compatible with all major browsers, enabling seamless deployment of AI applications across platforms.  
  - *Why it matters:* This development simplifies AI integration into web applications, reducing fragmentation and improving accessibility for developers.  
  - Post link: [WebGPU Finally, it is compatible with all major browsers](https://www.reddit.com/comments/1pbs9u9) (Score: 305, Comments: 41)

#### **Industry Developments**
- **[Work Culture at Deepseek](https://www.reddit.com/comments/1pbek1o)**  
  - Deepseek's leadership under Liang emphasizes flexible hours, collaboration, and avoiding the 996 work culture common in Chinese tech. The company attracts top talent with a supportive environment.  
  - *Why it matters:* This reflects a shift in AI industry culture, prioritizing innovation over exploitation, and could influence other companies to adopt similar practices.  
  - Post link: [Work culture at Deepseek](https://www.reddit.com/comments/1pbek1o) (Score: 800, Comments: 83)

### **2. Weekly Trend Comparison**

- **Persistent Trends**:  
  - DeepSeek's dominance continues, with its models consistently topping discussions due to their performance and affordability.  
  - Interest in long-context training and fine-tuning remains high, reflecting the community's focus on efficiency and accessibility.  

- **New Developments**:  
  - The 500K context length fine-tuning breakthrough and WebGPU compatibility are new and emerged in the last 24 hours, showcasing rapid progress in technical capabilities.  
  - DeepSeek's pricing strategy and competition performance are new focal points, shifting attention from previous weekly trends like Yann LeCun's historical contributions or Elon Musk's AGI predictions.  

- **Shifts in Interest**:  
  - The community is moving from theoretical discussions (e.g., AGI timelines) to practical advancements in model affordability, performance, and accessibility.  

### **3. Monthly Technology Evolution**

- **Progress in Model Performance**:  
  - Over the past month, DeepSeek has consistently improved its models, culminating in the V3.2-Speciale's gold medal performance in competitions. This reflects a focus on both performance and affordability.  

- **Open Source and Accessibility**:  
  - The emphasis on open-source models and local training (e.g., 500K context fine-tuning) highlights a broader trend toward democratizing AI technology.  

- **Web and Browser Integration**:  
  - WebGPU's browser compatibility is the latest step in integrating AI into web applications, building on earlier developments like Heretic for censorship removal.  

- **Work Culture and Innovation**:  
  - Discussions about work culture at Deepseek indicate a growing focus on sustainable innovation practices, contrasting with the 996 culture prevalent in other Chinese tech companies.  

### **4. Technical Deep Dive: 500K Context Length Fine-Tuning**

The most significant technical development today is the breakthrough in 500K context length fine-tuning, achieved through novel algorithms in Unsloth. This advancement allows training models like GPT-OSS-20B to achieve 6.4x longer contexts while using 72% less VRAM.  

- **Technical Details**:  
  - The method combines fused and chunked cross-entropy loss with enhanced activation offloading in gradient checkpointing.  
  - On an 80GB H100 GPU, the model achieves 500K+ context windows, compared to the previous limit of 80K.  

- **Why It Matters Now**:  
  - This innovation democratizes long-context training, enabling smaller organizations and individuals to achieve state-of-the-art results without massive computational resources.  
  - The reduction in VRAM usage (from 80GB to 60GB for 500K contexts) makes local training more accessible, reducing reliance on cloud services.  

- **Implications**:  
  - Wider adoption of long-context models in applications like legal document analysis, medical research, and complex problem-solving.  
  - Potential for new tools and services built on locally trained models, fostering innovation outside of large corporations.  

- **Community Insights**:  
  - Developers praised the efficiency gains, with one user noting, "Without your work, small-budget training would be 2 years behind where it is today."  
  - Discussions also touched on future applications, such as integrating these models into web applications via WebGPU.  

This breakthrough represents a significant shift in the AI ecosystem, enabling more efficient and accessible long-context training and fine-tuning.  

### **5. Community Highlights**

#### **r/singularity**  
- Focused on DeepSeek's dominance in competitions and affordability, with posts like "[Deepseek New Model gets Gold in IMO](https://www.reddit.com/comments/1pbaqkk)" and "[That is actually cheap damn](https://www.reddit.com/comments/1pbd84l)."  
- Discussions also touched on work culture at Deepseek, highlighting its innovative management practices.  

#### **r/LocalLLaMA**  
- Centered on technical advancements like the 500K context length fine-tuning and WebGPU compatibility.  
- Posts like "[deepseek-ai/DeepSeek-V3.2 · Hugging Face](https://www.reddit.com/comments/1pb9xm3)" and "[You can now do 500K context length fine-tuning](https://www.reddit.com/comments/1pbh87f)" dominated discussions, showcasing the community's focus on practical tools and resources.  

#### **Cross-Cutting Topics**  
- Both communities discussed DeepSeek's models, but r/singularity focused on their competitive performance, while r/LocalLLaMA emphasized technical capabilities and accessibility.  
- The topic of open-source models and their impact on democratizing AI was a common theme across communities.  

#### **Unique Discussions**  
- In r/singularity, the post "[Work culture at Deepseek](https://www.reddit.com/comments/1pbek1o)" sparked debate about sustainable work practices in the AI industry.  
- In r/LocalLLaMA, the discussion around the Artificial Analysis Openness Index highlighted the growing importance of transparency in AI development.  

These insights reflect a community focused on both the technical and ethical advancements in AI, with a growing emphasis on accessibility and sustainability.