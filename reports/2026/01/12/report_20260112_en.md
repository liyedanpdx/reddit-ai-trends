# Reddit AI Trend Report - 2026-01-12

## Today's Trending Posts

| Title | Community | Score | Comments | Category | Posted |
|-------|-----------|-------|----------|----------|--------|
| [LLM trained from scratch on 1800s London texts (1.2B para...](https://www.reddit.com/comments/1qaawts) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 634 | 73 | Other | 2026-01-11 21:00 UTC |
| [I bought a €9k GH200 “desktop” to save $1.27 on Claude Co...](https://www.reddit.com/comments/1qa1guo) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 541 | 137 | Tutorial | Guide | 2026-01-11 15:01 UTC |
| [Leader of Qwen team says Chinese companies severely const...](https://www.reddit.com/comments/1qa0kf8) | [r/singularity](https://www.reddit.com/r/singularity) | 311 | 114 | AI | 2026-01-11 14:23 UTC |
| [It works! Abliteration can reduce slop without training](https://www.reddit.com/comments/1qa0w6c) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 306 | 100 | Resources | 2026-01-11 14:37 UTC |
| [Another Erdos problem down!](https://www.reddit.com/comments/1q9wo1w) | [r/singularity](https://www.reddit.com/r/singularity) | 285 | 76 | AI | 2026-01-11 11:01 UTC |
| [Leader of Qwen team says Chinese companies severely const...](https://www.reddit.com/comments/1qa0ph9) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 272 | 90 | Discussion | 2026-01-11 14:29 UTC |
| [Qwen cutoff date makes our current reality too dystopian ...](https://www.reddit.com/comments/1qagaaq) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 167 | 71 | Funny | 2026-01-12 00:38 UTC |
| [Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs...](https://www.reddit.com/comments/1q9xn78) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 156 | 35 | News | 2026-01-11 12:00 UTC |
| [Elon Musk’s xAI tells investors it will build AI for Tesl...](https://www.reddit.com/comments/1qa45sf) | [r/singularity](https://www.reddit.com/r/singularity) | 144 | 36 | Discussion | 2026-01-11 16:47 UTC |
| [CES 2026 shows Humanoid robots moving from demos to real ...](https://www.reddit.com/comments/1qa0yz9) | [r/singularity](https://www.reddit.com/r/singularity) | 144 | 45 | Robotics | 2026-01-11 14:40 UTC |


## Weekly Popular Posts

| # | Title | Community | Score | Comments | Category | Posted |
|---|-------|-----------|-------|----------|----------|--------|
| 1 | [Atlas ends this year’s CES with a backflip](https://www.reddit.com/comments/1q8uk7o) | [r/singularity](https://www.reddit.com/r/singularity) | 4268 | 360 | Robotics | 2026-01-10 04:28 UTC |
| 2 | [The reason why RAM has become so expensive](https://www.reddit.com/comments/1q8ckz0) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 4112 | 359 | Funny | 2026-01-09 16:18 UTC |
| 3 | [Atlas has its own moves](https://www.reddit.com/comments/1q7q039) | [r/singularity](https://www.reddit.com/r/singularity) | 2335 | 228 | Robotics | 2026-01-08 22:19 UTC |
| 4 | [Boston Dynamics Atlas Demo](https://www.reddit.com/comments/1q5lr1h) | [r/singularity](https://www.reddit.com/r/singularity) | 2185 | 277 | Robotics | 2026-01-06 15:32 UTC |
| 5 | [When you using AI in coding](https://www.reddit.com/comments/1q6mxib) | [r/singularity](https://www.reddit.com/r/singularity) | 2026 | 151 | Meme | 2026-01-07 18:13 UTC |
| 6 | [We have reached THIS phase of android integration into so...](https://www.reddit.com/comments/1q4u8rt) | [r/singularity](https://www.reddit.com/r/singularity) | 1425 | 163 | Robotics | 2026-01-05 18:47 UTC |
| 7 | [True](https://www.reddit.com/comments/1q9bj1g) | [r/singularity](https://www.reddit.com/r/singularity) | 1251 | 38 | Meme | 2026-01-10 18:28 UTC |
| 8 | [one of the top submitters in the nvfp4 competition has ne...](https://www.reddit.com/comments/1q8clmf) | [r/singularity](https://www.reddit.com/r/singularity) | 1237 | 141 | AI | 2026-01-09 16:19 UTC |
| 9 | [Jensen Huang saying \"AI\" 121 times during the NVIDIA CE...](https://www.reddit.com/comments/1q7d8bj) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 893 | 144 | Tutorial | Guide | 2026-01-08 14:29 UTC |
| 10 | [I clustered 3 DGX Sparks that NVIDIA said couldn\'t be cl...](https://www.reddit.com/comments/1q8hqgd) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 842 | 140 | Resources | 2026-01-09 19:27 UTC |
| 11 | [Boston Dynamics & Google DeepMind Form New AI Partnership...](https://www.reddit.com/comments/1q4zwkl) | [r/singularity](https://www.reddit.com/r/singularity) | 803 | 96 | Robotics | 2026-01-05 22:13 UTC |
| 12 | [Report: Anthropic cuts off xAI’s access to Claude models ...](https://www.reddit.com/comments/1q8yzal) | [r/singularity](https://www.reddit.com/r/singularity) | 780 | 154 | Discussion | 2026-01-10 08:34 UTC |
| 13 | [Performance improvements in llama.cpp over time](https://www.reddit.com/comments/1q5dnyw) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 666 | 85 | Discussion | 2026-01-06 09:03 UTC |
| 14 | [DeepSeek-R1’s paper was updated 2 days ago, expanding fro...](https://www.reddit.com/comments/1q6c9wc) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 646 | 55 | Other | 2026-01-07 10:49 UTC |
| 15 | [LLM trained from scratch on 1800s London texts (1.2B para...](https://www.reddit.com/comments/1qaawts) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 631 | 73 | Other | 2026-01-11 21:00 UTC |
| 16 | [For the first time in 5 years, Nvidia will not announce a...](https://www.reddit.com/comments/1q4x5e9) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 624 | 198 | News | 2026-01-05 20:31 UTC |
| 17 | [The NO FAKES Act has a \"Fingerprinting\" Trap that kills...](https://www.reddit.com/comments/1q7qcux) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 593 | 86 | News | 2026-01-08 22:33 UTC |
| 18 | [llama.cpp performance breakthrough for multi-GPU setups](https://www.reddit.com/comments/1q4s8t3) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 571 | 200 | News | 2026-01-05 17:37 UTC |
| 19 | [VP of Research Leaves OpenAI](https://www.reddit.com/comments/1q4xe1d) | [r/singularity](https://www.reddit.com/r/singularity) | 562 | 158 | Discussion | 2026-01-05 20:40 UTC |
| 20 | [Alphabet Overtakes Apple, Becoming Second to Nvidia in Size](https://www.reddit.com/comments/1q7frna) | [r/singularity](https://www.reddit.com/r/singularity) | 552 | 47 | AI | 2026-01-08 16:08 UTC |


## Monthly Popular Posts

| # | Title | Community | Score | Comments | Category | Posted |
|---|-------|-----------|-------|----------|----------|--------|
| 1 | [Makeup is an art](https://www.reddit.com/comments/1pq4saw) | [r/singularity](https://www.reddit.com/r/singularity) | 4953 | 139 | Meme | 2025-12-18 22:50 UTC |
| 2 | [Atlas ends this year’s CES with a backflip](https://www.reddit.com/comments/1q8uk7o) | [r/singularity](https://www.reddit.com/r/singularity) | 4259 | 360 | Robotics | 2026-01-10 04:28 UTC |
| 3 | [The reason why RAM has become so expensive](https://www.reddit.com/comments/1q8ckz0) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 4103 | 359 | Funny | 2026-01-09 16:18 UTC |
| 4 | [Why can\'t the US or China make their own chips? Explained](https://www.reddit.com/comments/1pzn3iw) | [r/singularity](https://www.reddit.com/r/singularity) | 3001 | 516 | Compute | 2025-12-30 16:51 UTC |
| 5 | [\"Eternal\" 5D Glass Storage is entering commercial pilot...](https://www.reddit.com/comments/1pn9v03) | [r/singularity](https://www.reddit.com/r/singularity) | 2816 | 338 | Compute | 2025-12-15 15:15 UTC |
| 6 | [A really good point being made amid all the hate towards ...](https://www.reddit.com/comments/1ppa97p) | [r/singularity](https://www.reddit.com/r/singularity) | 2670 | 888 | Discussion | 2025-12-17 22:33 UTC |
| 7 | [Unitree H2 - jump side kick and moon kick](https://www.reddit.com/comments/1q3omv6) | [r/singularity](https://www.reddit.com/r/singularity) | 2582 | 443 | Robotics | 2026-01-04 12:21 UTC |
| 8 | [It’s over.&nbsp;GPT 5.2 aces one of the most important be...](https://www.reddit.com/comments/1ppynjo) | [r/singularity](https://www.reddit.com/r/singularity) | 2365 | 96 | Shitposting | 2025-12-18 18:45 UTC |
| 9 | [Atlas has its own moves](https://www.reddit.com/comments/1q7q039) | [r/singularity](https://www.reddit.com/r/singularity) | 2329 | 228 | Robotics | 2026-01-08 22:19 UTC |
| 10 | [Boston Dynamics Atlas Demo](https://www.reddit.com/comments/1q5lr1h) | [r/singularity](https://www.reddit.com/r/singularity) | 2194 | 277 | Robotics | 2026-01-06 15:32 UTC |
| 11 | [Realist meme of the year!](https://www.reddit.com/comments/1pqegcr) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 2128 | 125 | News | 2025-12-19 06:49 UTC |
| 12 | [When you using AI in coding](https://www.reddit.com/comments/1q6mxib) | [r/singularity](https://www.reddit.com/r/singularity) | 2016 | 151 | Meme | 2026-01-07 18:13 UTC |
| 13 | [Crazy true](https://www.reddit.com/comments/1pmfpka) | [r/singularity](https://www.reddit.com/r/singularity) | 2013 | 521 | AI | 2025-12-14 14:45 UTC |
| 14 | [Andrej Karpathy: Powerful Alien Tech Is Here---Do Not Fal...](https://www.reddit.com/comments/1pwhgre) | [r/singularity](https://www.reddit.com/r/singularity) | 1923 | 435 | AI | 2025-12-26 22:50 UTC |
| 15 | [sell me this pen](https://www.reddit.com/comments/1ppur15) | [r/singularity](https://www.reddit.com/r/singularity) | 1856 | 71 | Meme | 2025-12-18 16:13 UTC |
| 16 | [I\'m strong enough to admit that this bugs the hell out o...](https://www.reddit.com/comments/1pnfaqo) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 1806 | 393 | Funny | 2025-12-15 18:40 UTC |
| 17 | [Trump: \"We\'re gonna need the help of robots and other f...](https://www.reddit.com/comments/1pxhbg2) | [r/singularity](https://www.reddit.com/r/singularity) | 1794 | 498 | AI | 2025-12-28 03:46 UTC |
| 18 | [Prepare for an awesome 2026!](https://www.reddit.com/comments/1pspk5q) | [r/singularity](https://www.reddit.com/r/singularity) | 1789 | 160 | AI | 2025-12-22 03:43 UTC |
| 19 | [Gemini 3.0 Flash is out and it literally trades blows wit...](https://www.reddit.com/comments/1pp0abx) | [r/singularity](https://www.reddit.com/r/singularity) | 1730 | 328 | AI | 2025-12-17 16:02 UTC |
| 20 | [llama.cpp appreciation post](https://www.reddit.com/comments/1psbx2q) | [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA) | 1678 | 153 | Funny | 2025-12-21 17:28 UTC |


## Top Posts by Community (Past Week)

### r/AI_Agents

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [Do AI agents fail more because of bad reasoning or bad co...](https://www.reddit.com/comments/1qanpj6) | 24 | 11 | Discussion | 2026-01-12 06:34 UTC |
| [Why do most AI products still look like basic chat interf...](https://www.reddit.com/comments/1q9xm09) | 23 | 51 | Discussion | 2026-01-11 11:58 UTC |
| [Best stack for agentic workflow?](https://www.reddit.com/comments/1qaijd8) | 15 | 19 | Discussion | 2026-01-12 02:17 UTC |


### r/LocalLLaMA

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [LLM trained from scratch on 1800s London texts (1.2B para...](https://www.reddit.com/comments/1qaawts) | 634 | 73 | Other | 2026-01-11 21:00 UTC |
| [I bought a €9k GH200 “desktop” to save $1.27 on Claude Co...](https://www.reddit.com/comments/1qa1guo) | 541 | 137 | Tutorial | Guide | 2026-01-11 15:01 UTC |
| [It works! Abliteration can reduce slop without training](https://www.reddit.com/comments/1qa0w6c) | 306 | 100 | Resources | 2026-01-11 14:37 UTC |


### r/MachineLearning

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [\[R\] Why doubly stochastic matrix idea (using Sinkhorn-K...](https://www.reddit.com/comments/1qa0n65) | 89 | 26 | Discussion | 2026-01-11 14:26 UTC |
| [\[D\] During long training sessions, how do you manage to...](https://www.reddit.com/comments/1qa46hz) | 5 | 15 | Discussion | 2026-01-11 16:47 UTC |


### r/Rag

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [Could RAG as a service become a mainstream thing?](https://www.reddit.com/comments/1qa0dcg) | 1 | 11 | Discussion | 2026-01-11 14:15 UTC |


### r/singularity

| Title | Score | Comments | Category | Posted |
|-------|-------|----------|----------|--------|
| [Leader of Qwen team says Chinese companies severely const...](https://www.reddit.com/comments/1qa0kf8) | 311 | 114 | AI | 2026-01-11 14:23 UTC |
| [Another Erdos problem down!](https://www.reddit.com/comments/1q9wo1w) | 285 | 76 | AI | 2026-01-11 11:01 UTC |
| [Elon Musk’s xAI tells investors it will build AI for Tesl...](https://www.reddit.com/comments/1qa45sf) | 144 | 36 | Discussion | 2026-01-11 16:47 UTC |




## Trend Analysis

### **Today's Highlights**

#### **New Model Releases and Performance Breakthroughs**
- **LLM Trained on 1800s London Texts (1.2B Parameters)** - A user successfully trained a large language model using texts from 1800s London, resulting in a 1.2B parameter model. This unique dataset provides insights into historical language patterns and potential applications in historical research or creative writing.  
  *Why it matters:* This experiment showcases the versatility of LLMs in niche domains and the growing interest in training models on specialized datasets.  
  Post link: [LLM trained from scratch on 1800s London texts (1.2B para...](https://www.reddit.com/comments/1qaawts) (Score: 634, Comments: 73)

- **Abliteration Technique Reduces Model Uncertainty** - Researchers demonstrated that "Abliteration" can reduce model uncertainty ("slop") without additional training. This technique appears to target overused patterns and improve model consistency.  
  *Why it matters:* This innovation could enhance model reliability and reduce hallucinations, a critical challenge in LLM development.  
  Post link: [It works! Abliteration can reduce slop without training](https://www.reddit.com/comments/1qa0w6c) (Score: 306, Comments: 100)

#### **Industry Developments**
- **Chinese AI Companies Face Compute Constraints** - The leader of Alibaba's Qwen team highlighted significant compute resource limitations for Chinese AI companies compared to U.S. firms like OpenAI.  
  *Why it matters:* This underscores the global imbalance in AI research capabilities and could slow China's progress in achieving breakthroughs.  
  Post link: [Leader of Qwen team says Chinese companies severely const...](https://www.reddit.com/comments/1qa0kf8) (Score: 311, Comments: 114)

- **Gigabyte Announces DDR5-7200 CQDIMMs Support** - Gigabyte unveiled support for 256GB of DDR5-7200 CQDIMMs at CES 2026, offering faster memory speeds for AI workloads.  
  *Why it matters:* This advancement could improve hardware efficiency for local LLM setups and high-performance computing applications.  
  Post link: [Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs...](https://www.reddit.com/comments/1q9xn78) (Score: 156, Comments: 35)

#### **Research Innovations**
- **Erdős Problem Solving with AI** - Another Erdős problem was solved using AI, demonstrating the growing role of AI in mathematical research.  
  *Why it matters:* This highlights AI's potential to accelerate scientific discoveries and solve complex problems.  
  Post link: [Another Erdos problem down!](https://www.reddit.com/comments/1q9wo1w) (Score: 285, Comments: 76)

### **Weekly Trend Comparison**

- **Persistent Trends:** Robotics and AI hardware advancements remain prominent, with Boston Dynamics' Atlas and Gigabyte's DDR5 announcement continuing to draw attention.  
- **Newly Emerging Trends:** Today's posts introduce a stronger focus on LLM training techniques (e.g., Abliteration) and compute resource challenges, reflecting a shift toward technical optimizations and global competition in AI research.  

### **Monthly Technology Evolution**

- **From Robotics to LLM Optimizations:** Over the past month, the AI community has shifted focus from robotics advancements (e.g., Atlas demos) to more technical discussions around LLM training, hardware optimizations, and global compute resource disparities.  
- **Growing Interest in Specialized Models:** The training of LLMs on niche datasets (e.g., 1800s London texts) aligns with a broader trend of exploring specialized models for specific domains, indicating a maturation in the understanding of LLM capabilities.  

### **Technical Deep Dive: Abliteration Technique**

The Abliteration technique represents a novel approach to reducing model uncertainty ("slop") without requiring additional training. This method appears to target overused patterns in model outputs, potentially improving consistency and reducing hallucinations.  

- **Technical Details:** The technique seems to focus on identifying and mitigating repetitive or predictable patterns in model outputs, which are common issues in large language models.  
- **Significance:** Abliteration's ability to enhance model reliability without retraining could make it a valuable tool for fine-tuning and deploying models in production environments.  
- **Community Reaction:** Users expressed interest in applying this technique to other challenges, such as reducing overused phrases or improving creative writing outputs.  

### **Community Highlights**

- **r/LocalLLaMA:** The community is heavily focused on LLM training optimizations, hardware setups, and cost-saving strategies. Discussions around Abliteration and DDR5 support highlight a strong interest in technical advancements.  
- **r/singularity:** This community is exploring broader AI implications, including robotics deployments and the challenges faced by Chinese AI companies.  
- **Cross-Cutting Topics:** Compute resource constraints and LLM training innovations are gaining traction across communities, reflecting a shared interest in advancing AI capabilities.  

For more details, explore the posts directly:  
- [LLM trained from scratch on 1800s London texts (1.2B para...](https://www.reddit.com/comments/1qaawts)  
- [It works! Abliteration can reduce slop without training](https://www.reddit.com/comments/1qa0w6c)  
- [Leader of Qwen team says Chinese companies severely const...](https://www.reddit.com/comments/1qa0kf8)